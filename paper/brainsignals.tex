\documentclass[12pt,twoside]{article}
\usepackage{jmlda}

\newcommand{\sample}{\mathfrak{D}}
\newcommand{\Xmat}{\mathbf{X}}
\newcommand{\Ymat}{\mathbf{Y}}
\newcommand{\xfeat}{\boldsymbol{\chi}}
\newcommand{\yfeat}{\boldsymbol{\nu}}
\newcommand{\xobj}{\mathbf{x}}
\newcommand{\yobj}{\mathbf{y}}

\newcommand{\fmodel}{\mathbf{f}}
\newcommand{\Fmat}{\mathbf{F}}
\newcommand{\Thmat}{\boldsymbol{\Theta}}
\newcommand{\serrf}{S}

\newcommand{\fsset}{\mathcal{J}}
\newcommand{\fssub}{\mathcal{A}}
\newcommand{\fscrit}{Q}
\newcommand{\fsvec}{\mathbf{a}}
\newcommand{\BB}{\mathbb{B}}

\newcommand{\Sim}{\myop{Sim}}
\newcommand{\Rel}{\myop{Rel}}
\newcommand{\Qsim}{\mathbf{Q}}
\newcommand{\brel}{\mathbf{b}}

\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bc}{\mathbf{c}}

\newcommand{\bP}{\mathbf{P}}
\newcommand{\bT}{\mathbf{T}}

\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\var}{\textrm{var}}




%\NOREVIEWERNOTES
\title
    {Прогнозирование намерений по cигналам ECoG}
\author
    {Калиниченко~О.\,И., Ремизова~А.\,С.} % основной список авторов, выводимый в оглавление
\thanks
{	Научный руководитель:  Стрижов~В.\,В. 
	Задачу поставил:  Стрижов~В.\,В.
	Консультант:  Исаченко~Р.\,В.}
% \email{author@site.ru}
\organization
{$^1$ Московский физико-технический институт}
\abstract
{	Работа посвящена построению системы тестирования прогностических моделей, предлагающей различные критерии качества.
	Рассматривается случай коррелированных входных и выходных сигналов электрокортикограммы и фазовых траекторий движения конечностей высокой размерности.
	Ставится задача предсказания намерений по сигналам головного мозга.
	Входные данные -- сигналы электрокортикограммы (ECoG).
	Для выявления и устранения скрытых зависимостей используются методы снижения размерности пространства и отбора признаков.
	Предложенная система тестирования оценивает качество прогноза моделей и анализирует ошибку.
	Вычислительный эксперимент проводится на данных ECoG проекта NeuroTycho.
	
	\bigskip
	\textbf{Ключевые слова}: \emph {декодирование временных рядов, отбор признаков, PLS, QPFS, генетический алгоритм, BCI, электрокортикограмма, траектории движения конечностей}.}

\begin{document}

\maketitle

\section{Введение}
В работе исследуются методы моделирования нейросетевого интерфейса (BCI) \cite{Motrenko17ECoG}.
Входные данные -- сигналы мозга, полученные с помощью электрокортикографии (ECoG) и электроэнцефалографии (EEG). ECoG-сигналы имеют более высокое разрешение и большую амплитуду, однако для их получения требуется непосредственное подсоединение электродов к коре головного мозга. Одной из задач при построении систем BCI является предсказание намерений.

Предлагается декодировать исходные сигналы и спрогнозировать траекторию движения верхних конечностей. Исходное пространство имеет избыточно высокую размерность. Линейная зависимость между признаками приводит к мультиколлинеарности. Для устранения мультиколлинеарности предлагается применить методы понижения размерности и отбора признаков.

Признаковое описание многомерного временного ряда существует в пространствах независимых и зависимых переменных. Для учета существующих закономерностей в исходном и выходном пространстве используется скрытое пространство латентных переменных.  В скрытом пространстве происходит согласование между образами исходных пространств.

В эксперименте рассматриваются следующие модели: линейная регрессия, метод частных наименьших квадратов (PLS) \cite{Isachenko17PLS}, а также методы отбора признаков с помощью квадратичного программирования (QPFS) \cite{Katrutsa17QPFS} и генетического алгоритма.

Предлагается система тестирования прогностических моделей с оценкой качества и анализом ошибки. Подобный инструмент может применяться не только в задаче анализа сигналов мозга, но и во многих других задачах, связанных с прогнозированием многомерных временных рядов.



\section{Постановка задачи}
\paragraph{Задача предсказания}

Задана выборка $\sample = \left( \Xmat, \Ymat \right)$,
где $\Xmat = \left[ \xfeat_1 \dots \xfeat_n \right] \in \RR^{m \times n}$ --- матрица объектов,
$\Ymat = \left[ \yfeat_1 \dots \yfeat_r \right] \in \RR^{m \times r}$ --- целевая матрица .
Вектор $\xfeat_j \in \RR^{m}$ --- значения $j$-го признака на элементах выборки; 
вектор $\yfeat_i \in \RR^{m}$ --- $i$-й целевой столбец.
Имеется линейная модель $\fmodel(\xobj, \Thmat)$ с набором параметров $\Thmat \in \RR^{n \times r}$, аппроксимирующая целевой вектор $\yobj \in \RR^r$ по известным значениям признаков объекта $\xobj \in \RR^n$:

\begin{equation}
\label{eq::linear_model}
\fmodel(\xobj, \Thmat) =
\underset{r \times n}{\Thmat\T}
\underset{n \times 1}{\xobj}.
\end{equation}
За $\Fmat(\Xmat, \Thmat)$ обозначена матрица $[
\fmodel(\xobj_1, \Thmat), \dots, \fmodel(\xobj_m, \Thmat)]\T$.

На выборке $\mathfrak{D}$ и модели $\fmodel(\xobj, \Thmat)$ задана функция ошибки $\serrf$.
Задачей является поиск наилучших параметров $\Thmat$,
доставляющих минимум функции ошибки $\serrf$:
\begin{equation}
\label{eq::param_problem}
\Thmat^* =
\argmin_{\Thmat \in \RR^{m \times r}} 
\serrf(\Thmat | \sample, \fmodel).
\end{equation}

При наличии мультикоррелированных столбцов в матрице $\Xmat$, что вероятно, если пространство признаков имеет высокую размерность, решение задачи \eqref{eq::param_problem}
выбора матрицы параметров $\Thmat$ нестабильно. Определение устойчивости (стабильности) модели приведено в разделе <<Анализ ошибки>>.

\paragraph{Задача отбора признаков}

Пусть $\fsset = \{ 1, \dots, n \}$ --- множество индексов признаков,
$\fssub \subseteq \fsset$ --- его некоторое подмножество.
Определим линейную модель $\fmodel$ на подмножестве признаков $\fssub$:

\begin{equation}
\label{eq:fs_model}
\fmodel(
\xobj_{\fssub},
\Thmat_{\fssub}) =
\underset{r \times |\fssub|}{\Thmat_{\fssub}\T}
\underset{|\fssub| \times 1}{\xobj_{\fssub}}.
\end{equation}

Задача предсказания \eqref{eq::param_problem} состоит в определении матрицы параметров $\Thmat \in \RR^{|\fssub| \times r}$ при заданном множестве $\fssub$ такой, что:
	
\begin{equation}
\label{eq::fs_param_problem}
\Thmat^* =
\argmin_{\Thmat  \in \RR^{|\fssub| \times r}}
\serrf (\theta, \fssub | \sample, \fmodel).
\end{equation}

Для нахождения множества $\fssub$ решается задача отбора признаков:
\begin{equation}
\label{eq::fs_problem}
\fssub^* =
\argmin_{\fssub \in \fsset} 
\fscrit (\fssub | \Xmat, \Ymat), 
\end{equation}
где $\fscrit : \fssub \to \RR$ ---  некоторый функционал, который определяет качество выбранного подмножества признаков $\fssub \subseteq \fsset$.

Для решения этой задачи не обязательно требуется оценка оптимального вектора параметров $\Thmat^*$.
Она использует зависимости между векторами $\xfeat_j, j \in \fsset$ и целевыми векторами $\yfeat_i, i \in \{1, \dots, r \}$.

Пусть $\fsvec \in \BB^n = \{0, 1\}^n$ -- вектор принадлежности признаков подмножеству $\fssub$.
Задача $\eqref{eq::fs_problem}$ представлена в виде:
\begin{equation}
\label{eq::fs_vec_problem}
\fsvec^* = 
\argmin_{\fsvec \in \BB^n}
\fscrit(\fsvec | \Xmat, \Ymat), 
\end{equation}
где $\fscrit : \BB^n \to \RR$ -- критерий $\fscrit$, определенный на $ \BB^n $;
связь между вектором $\fsvec^*$ и множеством $\fssub^*$ определяется как:
\begin{equation}
\label{eq::fs_index_correspondence}
\fssub^* =
\{j:
\fsvec^*_j = 1,
j \in \fsset\}. 
\end{equation}


\section {Модели}

В данной работе рассматриваются модели линейной регрессии и PLS (Partial Least Squares Regression).

\paragraph{Линейная регрессия}

Модель линейной регрессии является линейной моделью \eqref{eq::linear_model}.
Матрица параметров $\Thmat \in \RR^{n \times r}$ подбирается таким образом, чтобы минимизировать квадратичную функцию ошибки $\serrf$ \eqref{eq::param_problem}:
\begin{equation}
\label{eq::linreg_error}
{\bigl\|
\bF(\Xmat, \Thmat) -
\Ymat \bigr\|}_2^2
\rightarrow 
\min_{
\Thmat \in \RR^{n \times r}}.
\end{equation}

Решением оптимизационной задачи является:
\begin{equation}
\label{eq::linreg_solution}
\Thmat =  (\Xmat\T \Xmat) ^{-1} \Xmat\T \Ymat.
\end{equation}




\paragraph{PLS}

Метод частных наименьших квадратов PLS рассматривает в качестве признаков линейные комбинации исходных. Предполагается, что существует скрытое пространство латентных переменных малой размерности $l$ ($l < n, r$). Происходит поиск матрицы $\bT \in \RR^{m \times l}$, которая наилучшим образом описывает матрицы $\Xmat$ и $\Ymat$.

\begin{align}
\underset{m \times n}{\Xmat} &= \underset{m \times l}{\bT\T} \cdot
\underset{l \times n}{\bP\T} + \underset{m \times n}{\bE}
\label{eq::PLS_X} \\
\underset{m \times r}{\Ymat} &= \underset{m \times l}{\bU\T} \cdot
% \underset{l \times l}{\textrm{diag}(\bbeta)}\cdot
\underset{l \times r}{\Qsim\T} + \underset{m \times r}{\bF}
\label{eq::PLS_Y}
\end{align}

\section{Методы отбора признаков}

В данной работе рассматриваются метод отбора признаков QPFS (Quadratic Programming Feature Selection) и генетический алгоритм.

\paragraph{QPFS}

Идея метода отбора признаков состоит в минимизации числа схожих признаков и максимизации числа признаков, информативных для задачи предсказания целевого вектора $\yfeat \in \RR^{m}$. Формализация постановки задачи состоит в ее определении как задачи \eqref{eq::fs_vec_problem} со следующим критерием $\fscrit$:
\begin{equation}
\label{eq::qpfs_crit}
\fscrit(\fsvec | \Xmat, \yfeat) = 
\fsvec\T \Qsim \fsvec - \brel \T \fsvec,
\end{equation}
где $\Qsim \in \RR^{m \times m}$ --- матрица попарного сходства признаков, $\brel \in \RR^m$ --- вектор релевантностей признаков целевому вектору.

Для вычисления матрицы $\Qsim$ и вектора $\brel$ определяются следующие функции:
\begin{align}
\label{eq::Sim}  \Sim : \; & \fsset \times \fsset \to [0, 1]; \\
\label{eq::Rel} \Rel: \;& \fsset \to [0, 1],
\end{align}
где функция $\Sim$ отражает схожесть между парой признаков, а функция $\Rel$ --- релевантность признака целевому вектору.

Тогда матрица $\Qsim$ вычисляется с использованием функции $\Sim$:
\begin{align}
\label{eq::Qsim_compute}
\Qsim = [q_{ij}]
= \Sim(\xfeat_i, \xfeat_j),
\end{align}
а вектор $\brel$ --- с использованием функции $\Rel$:
\begin{align}
\label{eq::brel_compute} \brel = [b_{i}] = \Rel(\xfeat_i).
\end{align}

Функции $\Sim$, $\Rel$ выбираются в соответствии с решаемой задачей. Далее рассматриваются некоторые примеры таких функций.

Схожесть между парой признаков можно вычислить, используя коэффициент корреляции Пирсона для случайных величин $\xfeat_i$, $\xfeat_j$:

\begin{equation}
\label{eq::pearson_corr}
\rho_{ij} =\frac{
	\cov(\xfeat_i, \xfeat_j)
}{
	\sqrt{\var(\xfeat_i) \cdot \var(\xfeat_j)}
},
\end{equation}
где $\cov$ --- ковариация случайных величин, $\var$ --- вариация случайной величины.

Пусть $\overline{\xfeat}_i = [\overline{\chi}_i  \dots \overline{\chi}_i] \in \RR^m$ --- вектор, значения элементов которому равны среднему $\overline{\chi}_i$ вектора $\xfeat_i$. Выборочный коэффициент корреляции Пирсона для векторов $\xfeat_i$, $\xfeat_j$ определяется как:
\begin{equation}
\label{eq::pearson_sample_corr}
\hat{\rho}_{ij} =\frac{
	(\xfeat_i - \overline{\xfeat}_i)\T (\xfeat_j - \overline{\xfeat}_j)
}{
	\lVert \xfeat_i  - \overline{\xfeat}_i \rVert_2
	\lVert \xfeat_j  - \overline{\xfeat}_j \rVert_2}
\end{equation}
Полагая $\xfeat_y = \yfeat$, можно аналогичным образом вычислить выборочный коэффициент корреляции Пирсона $\hat{\rho}_{iy}$ для вектора $\xfeat_i$ значений признака и целевого вектора $\yfeat$, чтобы измерить релевантность $i$-го признака целевому вектору.

Таким образом, элементы матрицы $\Qsim = [q_{ij}]$ и вектора $\brel = [b_{i}]$ вычисляются как:
\begin{align}
\label{eq::Qsim_pearson_corr}
	q_{ij} & = \Sim(\xfeat_i, \xfeat_j) = |\hat{\rho}_{ij}|; \\ 
\label{eq::brel_pearson_corr}
	b_i & = \Rel(\xfeat_i) = |\hat{\rho}_{iy}|.
\end{align}

Пусть $p(\xfeat_i)$ --- плотность случайной величины $\xfeat_i$,  $p(\xfeat_i, \xfeat_j)$ --- совместная плотность случайных величин $\xfeat_i$ и $ \xfeat_j$.
Для измерения нелинейной зависимости между случайными величинами $\xfeat_i$ и $\xfeat_j$ применяется взаимная информация:
\begin{equation}
	I(\xfeat_i, \xfeat_j) = \int \int p(\xfeat_i, \xfeat_j)
	\log \frac{p(\xfeat_i, \xfeat_j)}{p(\xfeat_i) p(\xfeat_j)}
	d\xfeat_i d\xfeat_j.
\end{equation}

Выборочная взаимная информация $\hat{I}(\xfeat_i, \xfeat_j)$ вычисляется на основе оценок соответствующих плотностей распределений.

Тогда элементы матрицы $\Qsim = [q_{ij}]$ и вектора $\brel = [b_{i}]$ вычисляются как:
\begin{align}
\label{eq::Qsim_mutual_info}
q_{ij} & = \Sim(\xfeat_i, \xfeat_j) = \hat{I}(\xfeat_i, \xfeat_j); \\ 
\label{eq::brel_mutual_info}
b_i & = \Rel(\xfeat_i) = \hat{I}(\xfeat_i, \yfeat).
\end{align}

\paragraph{Генетический алгоритм}
\[
\]

Изначально имеется "популяция" прогностических моделей одной и той же природы, но построенных с участием различных множеств признаков. Каждая модель характеризуется множеством признаков, которые будут использоваться при ее построении. Назовем это множество признаковым множеством модели. Начальный набор моделей выбирается случайным образом.

Алгортим состоит из итеративно повторяемых шагов. На каждом шаге происходит отбор, скрещивание и мутация. 
Обозначим $P_k$ - популяция на шаге k, $f$ — число лучших моделей в популяции, $g$ — число
моделей для скрещивания, $p$ — вероятность выбора модели для
мутации, $R(\fmodel, D)$ - метрика качества.
На каждом шаге алгоритм выполняет следующие действия:
\begin{enumerate}
	\item Отбор. Из $P_k$ выбирается f моделей с наименьшим значением метрики K. 
	\begin{equation}
	F = f_min_values(R(\fmodel| \mathfrak{D})).
	\label{eq::error_genetic}
	\end{equation}
	
	Они добавляются в $P_{k+1}$.
	\item Из множества F cлучайным образом выбираются g моделей для скрещивания.
	\item Скрещивание. Выбранные модели случайным образом разбиваются на пары.
	Для каждых моделей в одной паре происходит процедура "скрещивания": формируется новое множество признаков, в котором каждый признак присутствует, если он есть в признаковом множестве обеих моделей-родителей, не присутствует, если его нет в обоих признаковых множествах моделей-родителей, и присутствует с вероятностью $\frac{1}{2}$, если он есть ровно в одном из признаковых множеств.
	Модели, полученные с помощью скрещивания (их $\frac{g}{2}$), добавляюся в $P_{k+1}$
	\item Мутация. Каждая модель из $P_{k+1}$ подвергается мутации: каждый элемент ее признакового множества с вероятностью $p$ удаляется из множества, каждый признак не из ее признакового множества с вероятностью $p$ добавляется в множество. Таким образом получается новое признаковое множество. Вероятность $p$ обычно выбирают небольшой.
\end{enumerate}

Такие шаги происходят фиксированное число раз или до выполнения какого-либо заданного критерия останова. Выбор критерия, а также другие детали, зависят от реализации.

Ответом алгоритма является признаковое множество модели с наименьшим значением метрики, полученной на каком-либо шаге алгоритма.


\section{Общее описание задачи анализа ошибки}

В данной работе особое внимание уделяется исследованию не только точности, но и устойчивости алгоритма отбора признаков и построенной на отобранном множестве признаков модели. 

Устойчивостью модели будем называть отсутствие статистически значимых изменений предсказаний при статистически незначимых изменениях выборки или множества признаков. 

Для изучения устойчивости модели предлагается исследовать поведение функции ошибки при изменении выборки.

Введем некоторые обозначения.

Напомним, 
$\fssub \subseteq \fsset$ --- некоторое подмножество множества признаков.

$\Thmat^* = \argmin_{\Thmat  \in \RR^{|\fssub| \times r}}   S(\theta, \fssub | \mathfrak{D}, \fmodel) $ 
- предсказанные линейной моделью коэффициенты.

Пусть
$\hat{y}  = X\Thmat \in R^{m \times r}$ - предсказания линейной модели.

$\epsilon = y - \hat{y}$ - ошибка предсказания

$N$ - множество отобранных множеств признаков (см. Методы отбора признаков).

$L$ - множество исследуемых типов моделей (см. Модели)

$M$ - множество метрик качества модели (см. ниже в секции Метрики качества)

Для каждой пары алгоритм-тип модели можно построить модель данного типа, использующую отобранный данным алгоритмом набор признаков.

Для того, чтобы вызвать незначительные изменения выборки, используется процедура бутстрепа. Мы будем создавать новые выборки, выбирая c возвращением m элементов исходной выборки. Фактически, таким образом будут генерироваться выборки из эмпирического распределения исходных данных.

Пусть $k$ - количество бутстрепных выборок. $K$- множество бутстрепных выборок.

Для изменения множества параметров используется метод семплирования пространства параметров. Выбираются случайные подвыборки размера r < n. ??? мы этого не делаем вроде, пусть пока лежит.

Для каждого элемента из $N \times L \times K$ считается ошибка линейной модели при применении данного алгоритма отбора признаков. Можно считать, что полученная выборка сгенерирована из некоторого распределения функции ошибки. Задача анализа ошибки состоит в исследовании свойств данного распределения. Более подробно это будет описано в следующей секции (пока не придумала название).

В данной работе особой внимание уделяется исследованию зависимости устойчивости алгоритма от количества отобранных признаков. Для этого каждый алгоритм отбора признаков реализован таким образом, чтобы имелась возможность задавать желаемое количество отобранных признаков как гипер-параметр. Таким образом, в исследовании зависимости устойчивости модели от количества признаков, $N = Alg \times \mathcal{R}_m$, где $Alg$ - множество исследуемых алгоритмов отбора признаков, $\mathcal{R}_m = \{1, 2, ..., n\}$

\section {Анализ распределения ошибок}


\section{Метрики} 

Характеристикой является сложность

Пусть имеются истинный прогноз $\Ymat = (\yobj_1, \yobj_2, \dots, \yobj_m)$ и предсказание $\mathbf{\hat{Y}} = (\mathbf{\hat{y}}_1, \mathbf{\hat{y}}_2, \dots, \mathbf{\hat{y}}_m)$; $\yobj_i$ и $\mathbf{\hat{y}}_i$, $i = 1, 2, \dots, m$ -- вектора размерности $r$.

Среднеквадратичная  ошибка (mean squared error):

\begin{equation}
\text{MSE}(\Ymat, \mathbf{\hat{Y}}) =
\frac{1}{m}
\sum^m_{i=1}
{\bigl\| \mathbf{y}_i - \mathbf{\hat{y}}_i \bigr\| }_2^2 
\label{eq::error_mse}
\end{equation}

Корень среднеквадратичной ошибки (root-mean-squared error):

\begin{equation}
\text{RMSE}(\Ymat, \mathbf{\hat{Y}}) =
\sqrt{\text{MSE}(\Ymat, \mathbf{\hat{Y}})}
\label{eq::error_rmse}
\end{equation}

Нормированная среднеквадратичная ошибка (scaled mean squared error):

\begin{equation}
\text{sMSE}(\Ymat, \mathbf{\hat{Y}}) =
\frac{
	\sum^m_{i=1}
	{\bigl\| \mathbf{y}_i - \mathbf{\hat{y}}_i \bigr\| }_2^2 
}{
	\sum^m_{i=1}
	{\bigl\| \mathbf{y}_i - \mathbf{\overline{y}} \bigr\| }_2^2 
}, \;
\mathbf{\overline{y}} = \frac{1}{m} \sum_{i=1}^{m} y_i
\label{eq::error_smse}
\end{equation}

\paragraph{Мультиколлинеарность}
Предположим, что приз


Число обусловленности матрицы $\Xmat\T
\Xmat$ 


\begin{equation}
\kappa =  \frac{\lambda_{\textrm{max}}}{\lambda_{\textrm{min}}}
\end{equation}

Рассматриваются числа обусловленности для $\Xmat\T \Xmat$, $\widehat{\Ymat}\T \widehat{\Ymat}$. В случае применения модели PLS, будут рассматриваться также числа обусловленности $\bT\T \bT$


MAE, MADE, Коэффициент корреляции. Различные параметры модели из \cite{Katrutsa17QPFS}.

Сложность модели

\begin{thebibliography}{1}
\bibitem{bci}
    \BibAuthor{
    	J. del R.\; Mill?n,
    	F. \; Renken,
    	J. \; Mouri?o and
    	W. \; Gerstner}
    \BibTitle{Brain-actuated interaction}~//
    \BibJournal{Artif. Intell.}, 159(2004) 241–259.
\bibitem{Isachenko17PLS}
    \BibAuthor{
    	Isachenko \; R.,
    	Vladimirova \; M.,
    	Strijov \; V.}
    \BibTitle{Dimensionality reduction for time series decoding and
    	forecasting problems}~//
    \BibJournal{Machine Learning and Data Analysis}.
\bibitem{Katrutsa15StressTest}
	\BibAuthor{
	Katrutsa \; A.,
	Strijov \; V.}
	\BibTitle{Stress test procedure for feature selection algorithms}, 2015~//
\BibJournal{Expert System with Applications}, 142, 172–183
\bibitem{Katrutsa17QPFS}
    \BibAuthor{
    	Katrutsa \; A.,
    	Strijov \; V.}
    \BibTitle{Comprehensive study of feature selection methods to solve
    	multicollinearity problem according to evaluation criteria}, 2017~//
    \BibJournal{Expert System with Applications}, 76, 1-11.
\bibitem{Motrenko17ECoG}
	\BibAuthor{
		Motrenko \; A.,
		Strijov \; V.}
	\BibTitle{Multi-way Feature Selection for ECoG-based Brain-Computer
	Interface}~//
	\BibJournal{???}.

\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
