\documentclass[12pt,twoside]{article}
\usepackage{jmlda}

\newcommand{\sample}{\mathfrak{D}}
\newcommand{\xmat}{\mathbf{X}}
\newcommand{\ymat}{\mathbf{Y}}
\newcommand{\xfeat}{\boldsymbol{\chi}}
\newcommand{\yfeat}{\boldsymbol{\nu}}
\newcommand{\xobj}{\mathbf{x}}
\newcommand{\yobj}{\mathbf{y}}

\newcommand{\fmodel}{\mathbf{f}}
\newcommand{\thmat}{\boldsymbol{\Theta}}
\newcommand{\serrf}{S}

\newcommand{\ba}{\mathbf{a}}

\newcommand{\bbeta}{\boldsymbol{\beta}}


\newcommand{\bw}{\mathbf{w}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bbb}{\mathbf{b}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\Sim}{\myop{Sim}}
\newcommand{\Rel}{\myop{Rel}}
\newcommand{\var}{\textrm{var}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\BB}{\mathbb{B}}

%\NOREVIEWERNOTES
\title
    {Прогнозирование намерений по cигналам ECoG}
\author
    {Калиниченко~О.\,И., Ремизова~А.\,С.} % основной список авторов, выводимый в оглавление
\thanks
{	Научный руководитель:  Стрижов~В.\,В. 
	Задачу поставил:  Стрижов~В.\,В.
	Консультант:  Исаченко~Р.\,В.}
% \email{author@site.ru}
\organization
{$^1$ Московский физико-технический институт}
\abstract
{	Работа посвящена построению системы тестирования прогностических моделей, предлагающей различные критерии качества.
	Рассматривается случай коррелированных входных и выходных сигналов электрокортикограммы и фазовых траекторий движения конечностей высокой размерности.
	Ставится задача предсказания намерений по сигналам головного мозга.
	Входные данные -- сигналы электрокортикограммы (ECoG).
	Для выявления и устранения скрытых зависимостей используются методы снижения размерности пространства и отбора признаков.
	Предложенная система тестирования оценивает качество прогноза моделей и анализирует ошибку.
	Вычислительный эксперимент проводится на данных ECoG проекта NeuroTycho.
	
	\bigskip
	\textbf{Ключевые слова}: \emph {декодирование временных рядов, отбор признаков, PLS, QPFS, генетический алгоритм, BCI, электрокортикограмма, траектории движения конечностей}.}

\begin{document}

\maketitle

\section{Введение}
В работе исследуются методы моделирования нейросетевого интерфейса (BCI) \cite{Motrenko17ECoG}.
Входные данные -- сигналы мозга, полученные с помощью электрокортикографии (ECoG) и электроэнцефалографии (EEG). ECoG-сигналы имеют более высокое разрешение и большую амплитуду, однако для их получения требуется непосредственное подсоединение электродов к коре головного мозга. Одной из задач при построении систем BCI является предсказание намерений.

Предлагается декодировать исходные сигналы и спрогнозировать траекторию движения верхних конечностей. Исходное пространство имеет избыточно высокую размерность. Линейная зависимость между признаками приводит к мультиколлинеарности. Для устранения мультиколлинеарности предлагается применить методы понижения размерности и отбора признаков.

Признаковое описание многомерного временного ряда существует в пространствах независимых и зависимых переменных. Для учета существующих закономерностей в исходном и выходном пространстве используется скрытое пространство латентных переменных.  В скрытом пространстве происходит согласование между образами исходных пространств.

В эксперименте рассматриваются следующие модели: линейная регрессия, метод частных наименьших квадратов (PLS) \cite{Isachenko17PLS}, а также методы отбора признаков с помощью квадратичного программирования (QPFS) \cite{Katrutsa17QPFS} и генетического алгоритма.

Предлагается система тестирования прогностических моделей с оценкой качества и анализом ошибки. Подобный инструмент может применяться не только в задаче анализа сигналов мозга, но и во многих других задачах, связанных с прогнозированием многомерных временных рядов.



\section{Постановка задачи}
\paragraph{Задача предсказания}

Задана выборка $\sample = \left( \xmat, \ymat \right)$,
где $\xmat = \left[ \xfeat_1 \dots \xfeat_n \right] \in \RR^{m \times n}$ --- матрица объектов,
$\ymat = \left[ \yfeat_1 \dots \yfeat_r \right] \in \RR^{m \times r}$ --- целевая матрица .
Вектор $\xfeat_i \in \RR^{m}$ --- значения $i$-го признака на элементах выборки; 
вектор $\yfeat_j \in \RR^{m}$ --- $j$-й целевой столбец.
Имеется линейная модель $\fmodel(\xobj, \thmat)$ с набором параметров $\thmat \in \RR^{n \times r}$, аппроксимирующая $\yobj \in \RR^r$ по $\xobj \in \RR^n$:

\begin{equation}
\label{eq::linear_model}
\fmodel(\xobj, \thmat) =
\underset{r \times n}{\thmat\T}
\underset{n \times 1}{\xobj}.
\end{equation}

На выборке $\mathfrak{D}$  и модели $\fmodel(\xobj, \thmat)$ задана функция ошибки $\serrf$.
Задачей является поиск наилучших параметров $\thmat^*$,
доставляющих минимум функции ошибки $\serrf$:
\begin{equation}
\label{eq::param_problem}
\thmat^* =
\argmin_{\thmat \in \RR^{m \times r}} 
\serrf(\thmat | \sample, \fmodel).
\end{equation}

При наличии мультикоррелированных столбцов в матрице $\xmat$, что вероятно, если пространство признаков имеет высокую размерность, решение задачи \eqref{eq::param_problem}
выбора матрицы параметров $\thmat$ нестабильно. Определение устойчивости (стабильности) модели приведено в разделе "Анализ ошибки".

\paragraph{Постановка задачи отбора признаков}

Пусть $\cJ = \{ 1, \dots, n \}$ --- множество индексов признаков, а $\cA \subseteq \cJ$ --- его некоторое подмножество. Тогда линейную модель $\fmodel$ на подмножестве признаков $\cA$ можно определить как:

\begin{equation}
\fmodel (\xobj, \cA, \thmat) = \underset{1 \times |\cA|}{\xobj_{\cA}} \underset{|\cA| \times r}{\thmat} \label{eq:fs_model}
\end{equation}

Функция ошибки $S$ теперь зависит от выборки $\mathfrak{D}$, модели $\fmodel$ с параметрами $\thmat$ и от $\cA$.

Тогда задача предсказания определяется как поиск набора признаков $\cA$ и матрицы параметров $\thmat^* \in \RR^{|\cA| \times r}$ такого, что:
\begin{equation}
\thmat^* = \argmin_{\thmat  \in \RR^{|\cA| \times r}}   S(\theta, \cA | \mathfrak{D}, \fmodel)   \label{eq::fs_forecast}
\end{equation}


Задача отбора признаков поставлена как:
\begin{equation}
\cA^* = \argmin_{\cA \in cJ} Q(\cA | \xmat, \yobj), \label{eq::fs_problem}
\end{equation}

где $Q : \cA \to \RR$ --- это некоторый критерий качества,который определяет качество выбранного подмножества признаков $\cA \subseteq \cJ$.

Для решения этой задачи не обязательно требуется оценка оптимального вектора параметров $\thmat^*$. Она использует зависимости между векторами $\xfeat_j, j \in \cJ$ и целевыми векторами $y_i, i \in \{1, \dots, r \}$.

Пусть $\ba \in \BB^n = \{0, 1\}^n$ -- вектор индикаторов принадлежности признаков подмножеству $\cA$ -- то есть $\ba_j = 1$ тогда и только тогда, когда $j \in \cA$. Тогда задача $\ref{eq::fs_problem}$ может быть записана как:
\begin{equation}
\ba^* = \argmin_{\ba \in \BB^n} Q(\cA | \xmat, \yobj), \label{eq::fs_vec_problem}
\end{equation}
где $Q : \BB^n \to \RR$ -- критерий $Q$, но определенный на $ \BB^n $; связь между вектором $\ba^*$ и множеством $\cA^*$ определяется через:
\begin{equation}
\ba^*_j = 1 \Leftrightarrow j \in \cA^*, j \in \cJ. \label{eq::fs_index_correspondence}
\end{equation}


\section {Модели}

В данной работе используются две линейные модели:

\begin{enumerate}
	\item Линейная регрессия
	\item PLS(Partial Least Squares Regression)
\end{enumerate}
\paragraph{Линейная регрессия}

Модель линейной регрессии является линейной моделью

\begin{equation}
\fmodel(\xobj, \thmat) = \underset{1 \times n}{\xobj}\T \underset{n \times r}{\thmat}.
\label{eq::linear_regr}
\end{equation}

Параметр $\thmat$ подбирается таким образом, чтобы минимизировать квадратичную функцию ошибки

\begin{equation}
{\bigl\| \bF(\xmat, \thmat) - \ymat \bigr\| }_2^2
\rightarrow \min_{\thmat \in \RR^{n \times r}}
\label{eq::error_linear}
\end{equation}

Решение этой оптимизационной задачи можно выразить явно:

\begin{equation}
{\thmat =  (\xmat^T \xmat) ^{-1} \xmat^T \ymat }
\label{eq::linear_solution}
\end{equation}




\paragraph{PLS}

Метод частных наименьших квадратов PLS рассматривает в качестве признаков линейные комбинации исходных. Предполагается, что существует скрытое пространство латентных переменных малой размерности $l$ ($l < n, r$). Происходит поиск матрицы $\bT \in \RR^{m \times l}$, которая наилучшим образом описывает матрицы $\xmat$ и $\ymat$.

\begin{align}
\underset{m \times n}{\xmat} &= \underset{m \times l}{\bT\T} \cdot
\underset{l \times n}{\bP\T} + \underset{m \times n}{\bE}
\label{eq::PLS_X} \\
\underset{m \times r}{\ymat} &= \underset{m \times l}{\bU\T} \cdot
% \underset{l \times l}{\textrm{diag}(\bbeta)}\cdot
\underset{l \times r}{\bQ\T} + \underset{m \times r}{\bF}
\label{eq::PLS_Y}
\end{align}

\section{Методы отбора признаков}

Существуют различные методы отбора признаков. В данной работе используются:
\begin{enumerate}
	\item QPFS
	\item Генетический алгоритм
\end{enumerate}

Приведем краткие описания используемых алгоритмов.

\paragraph{QPFS}
\[
\]
Чтобы вычислить матрицу $\bQ$ и вектор $\bbb$

\begin{align}
\Sim & \label{eq::Sim} \\
\Rel & \label{eq::Rel}
\end{align}

Коэффициент корреляции Пирсона определяется как:

$$\rho_{ij} =\frac{
	\cov(\xobj_i, \xobj_j)
}{
	\sqrt{\var(\xobj_i) \cdot \var(\xobj_j)}
} $$


\paragraph{Генетический алгоритм}
\[
\]

Изначально имеется "популяция" прогностических моделей одной и той же природы, но построенных с участием различных множеств признаков. Каждая модель характеризуется множеством признаков, которые будут использоваться при ее построении. Назовем это множество признаковым множеством модели. Начальный набор моделей выбирается случайным образом.

Алгортим состоит из итеративно повторяемых шагов. На каждом шаге происходит отбор, скрещивание и мутация. 
Обозначим $P_k$ - популяция на шаге k, $f$ — число лучших моделей в популяции, $g$ — число
моделей для скрещивания, $p$ — вероятность выбора модели для
мутации, $R(\fmodel, D)$ - метрика качества.
На каждом шаге алгоритм выполняет следующие действия:
\begin{enumerate}
	\item Отбор. Из $P_k$ выбирается f моделей с наименьшим значением метрики K. 
	\begin{equation}
	F = f_min_values(R(\fmodel| \mathfrak{D})).
	\label{eq::error_genetic}
	\end{equation}
	
	Они добавляются в $P_{k+1}$.
	\item Из множества F cлучайным образом выбираются g моделей для скрещивания.
	\item Скрещивание. Выбранные модели случайным образом разбиваются на пары.
	Для каждых моделей в одной паре происходит процедура "скрещивания": формируется новое множество признаков, в котором каждый признак присутствует, если он есть в признаковом множестве обеих моделей-родителей, не присутствует, если его нет в обоих признаковых множествах моделей-родителей, и присутствует с вероятностью $\frac{1}{2}$, если он есть ровно в одном из признаковых множеств.
	Модели, полученные с помощью скрещивания (их $\frac{g}{2}$), добавляюся в $P_{k+1}$
	\item Мутация. Каждая модель из $P_{k+1}$ подвергается мутации: каждый элемент ее признакового множества с вероятностью $p$ удаляется из множества, каждый признак не из ее признакового множества с вероятностью $p$ добавляется в множество. Таким образом получается новое признаковое множество. Вероятность $p$ обычно выбирают небольшой.
\end{enumerate}

Такие шаги происходят фиксированное число раз или до выполнения какого-либо заданного критерия останова. Выбор критерия, а также другие детали, зависят от реализации.

Ответом алгоритма является признаковое множество модели с наименьшим значением метрики, полученной на каком-либо шаге алгоритма.


\section{Общее описание задачи анализа ошибки}

В данной работе особое внимание уделяется исследованию не только точности, но и устойчивости алгоритма отбора признаков и построенной на отобранном множестве признаков модели. 

Устойчивостью модели будем называть отсутствие статистически значимых изменений предсказаний при статистически незначимых изменениях выборки или множества признаков. 

Для изучения устойчивости модели предлагается исследовать поведение функции ошибки при изменении выборки.

Введем некоторые обозначения.

Напомним, 
$\cA \subseteq \cJ$ --- некоторое подмножество множества признаков.

$\thmat^* = \argmin_{\thmat  \in \RR^{|\cA| \times r}}   S(\theta, \cA | \mathfrak{D}, \fmodel) $ 
- предсказанные линейной моделью коэффициенты.

Пусть
$\hat{y}  = X\thmat \in R^{m \times r}$ - предсказания линейной модели.

$\epsilon = y - \hat{y}$ - ошибка предсказания

$N$ - множество отобранных множеств признаков (см. Методы отбора признаков).

$L$ - множество исследуемых типов моделей (см. Модели)

$M$ - множество метрик качества модели (см. ниже в секции Метрики качества)

Для каждой пары алгоритм-тип модели можно построить модель данного типа, использующую отобранный данным алгоритмом набор признаков.

Для того, чтобы вызвать незначительные изменения выборки, используется процедура бутстрепа. Мы будем создавать новые выборки, выбирая c возвращением m элементов исходной выборки. Фактически, таким образом будут генерироваться выборки из эмпирического распределения исходных данных.

Пусть $k$ - количество бутстрепных выборок. $K$- множество бутстрепных выборок.

Для изменения множества параметров используется метод семплирования пространства параметров. Выбираются случайные подвыборки размера r < n. ??? мы этого не делаем вроде, пусть пока лежит.

Для каждого элемента из $N \times L \times K$ считается ошибка линейной модели при применении данного алгоритма отбора признаков. Можно считать, что полученная выборка сгенерирована из некоторого распределения функции ошибки. Задача анализа ошибки состоит в исследовании свойств данного распределения. Более подробно это будет описано в следующей секции (пока не придумала название).

В данной работе особой внимание уделяется исследованию зависимости устойчивости алгоритма от количества отобранных признаков. Для этого каждый алгоритм отбора признаков реализован таким образом, чтобы имелась возможность задавать желаемое количество отобранных признаков как гипер-параметр. Таким образом, в исследовании зависимости устойчивости модели от количества признаков, $N = Alg \times \mathcal{R}_m$, где $Alg$ - множество исследуемых алгоритмов отбора признаков, $\mathcal{R}_m = \{1, 2, ..., n\}$

\section {Анализ распределения ошибок}


\section{Метрики} 

Характеристикой является сложность

Пусть имеются истинный прогноз $\ymat = (\yobj_1, \yobj_2, \dots, \yobj_m)$ и предсказание $\mathbf{\hat{Y}} = (\mathbf{\hat{y}}_1, \mathbf{\hat{y}}_2, \dots, \mathbf{\hat{y}}_m)$; $\yobj_i$ и $\mathbf{\hat{y}}_i$, $i = 1, 2, \dots, m$ -- вектора размерности $r$.

Среднеквадратичная  ошибка (mean squared error):

\begin{equation}
\text{MSE}(\ymat, \mathbf{\hat{Y}}) =
\frac{1}{m}
\sum^m_{i=1}
{\bigl\| \mathbf{y}_i - \mathbf{\hat{y}}_i \bigr\| }_2^2 
\label{eq::error_mse}
\end{equation}

Корень среднеквадратичной ошибки (root-mean-squared error):

\begin{equation}
\text{RMSE}(\ymat, \mathbf{\hat{Y}}) =
\sqrt{\text{MSE}(\ymat, \mathbf{\hat{Y}})}
\label{eq::error_rmse}
\end{equation}

Нормированная среднеквадратичная ошибка (scaled mean squared error):

\begin{equation}
\text{sMSE}(\ymat, \mathbf{\hat{Y}}) =
\frac{
	\sum^m_{i=1}
	{\bigl\| \mathbf{y}_i - \mathbf{\hat{y}}_i \bigr\| }_2^2 
}{
	\sum^m_{i=1}
	{\bigl\| \mathbf{y}_i - \mathbf{\overline{y}} \bigr\| }_2^2 
}, \;
\mathbf{\overline{y}} = \frac{1}{m} \sum_{i=1}^{m} y_i
\label{eq::error_smse}
\end{equation}

\paragraph{Мультиколлинеарность}
Предположим, что приз


Число обусловленности матрицы $\xmat\T
\xmat$ 


\begin{equation}
\kappa =  \frac{\lambda_{\textrm{max}}}{\lambda_{\textrm{min}}}
\end{equation}

Рассматриваются числа обусловленности для $\xmat\T \xmat$, $\widehat{\ymat}\T \widehat{\ymat}$. В случае применения модели PLS, будут рассматриваться также числа обусловленности $\bT\T \bT$


MAE, MADE, Коэффициент корреляции. Различные параметры модели из \cite{Katrutsa17QPFS}.

Сложность модели

\begin{thebibliography}{1}
\bibitem{bci}
    \BibAuthor{
    	J. del R.\; Mill?n,
    	F. \; Renken,
    	J. \; Mouri?o and
    	W. \; Gerstner}
    \BibTitle{Brain-actuated interaction}~//
    \BibJournal{Artif. Intell.}, 159(2004) 241–259.
\bibitem{Isachenko17PLS}
    \BibAuthor{
    	Isachenko \; R.,
    	Vladimirova \; M.,
    	Strijov \; V.}
    \BibTitle{Dimensionality reduction for time series decoding and
    	forecasting problems}~//
    \BibJournal{Machine Learning and Data Analysis}.
\bibitem{Katrutsa15StressTest}
	\BibAuthor{
	Katrutsa \; A.,
	Strijov \; V.}
	\BibTitle{Stress test procedure for feature selection algorithms}, 2015~//
\BibJournal{Expert System with Applications}, 142, 172–183
\bibitem{Katrutsa17QPFS}
    \BibAuthor{
    	Katrutsa \; A.,
    	Strijov \; V.}
    \BibTitle{Comprehensive study of feature selection methods to solve
    	multicollinearity problem according to evaluation criteria}, 2017~//
    \BibJournal{Expert System with Applications}, 76, 1-11.
\bibitem{Motrenko17ECoG}
	\BibAuthor{
		Motrenko \; A.,
		Strijov \; V.}
	\BibTitle{Multi-way Feature Selection for ECoG-based Brain-Computer
	Interface}~//
	\BibJournal{???}.

\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
